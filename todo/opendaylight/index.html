<!doctype html>
<html lang="en-us">
  <head>
    <title> // JiangX</title>
    <meta charset="utf-8" />
    <meta name="generator" content="Hugo 0.57.2" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="author" content="Jiang Xing" />
    <meta name="description" content="" />
    <link rel="stylesheet" href="http://example.org/css/main.min.59023e5fd38d6ecb0e1dfbb295077c3c67e00e3b9eb3feaf34b5a5e6b332897a.css" />

    
    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content=""/>
<meta name="twitter:description" content="Opendaylight 集群基础 架构 数据同步 分布式数据存储同步使用Raft，远程过程调用仓库同步使用Gossip
分部署数据存储(Distributed Data Store)  HA   Replication  a) 按序复制(Journal replication) b) 快照型复制(Snapshot replication)  Durability/Recovery   数据存储流程  Startup
向DistributedConfigDataProviderModule发起createInstance请求，创建实例并执行监听回调(waitTillReadyLatch)。由ShardManager向Shard中进行数据存储，当数据存储Ready后，waitTillReadyLatch收到ready完成create流程
Recovery
从硬盘读取最新状态反馈给ShardManager，全部完毕后，通知waitTillReadyLatch countDown
Recovery的条件：
a) Recovery必须是完整的
b) 所有的Shard Leaders必须已知
c) 三类消息被ShardManager监听
   消息名 说明     Cluster.MemberStatusUp 用于说明集群member的地址   LeaderStateChanged 用于说明follower拥有不同的leader   ShardRoleChanged 用于说明一个Shard的角色发生变化        d) 默认90s超时，但可配置
e) block config sub-system?"/>

    <meta property="og:title" content="" />
<meta property="og:description" content="Opendaylight 集群基础 架构 数据同步 分布式数据存储同步使用Raft，远程过程调用仓库同步使用Gossip
分部署数据存储(Distributed Data Store)  HA   Replication  a) 按序复制(Journal replication) b) 快照型复制(Snapshot replication)  Durability/Recovery   数据存储流程  Startup
向DistributedConfigDataProviderModule发起createInstance请求，创建实例并执行监听回调(waitTillReadyLatch)。由ShardManager向Shard中进行数据存储，当数据存储Ready后，waitTillReadyLatch收到ready完成create流程
Recovery
从硬盘读取最新状态反馈给ShardManager，全部完毕后，通知waitTillReadyLatch countDown
Recovery的条件：
a) Recovery必须是完整的
b) 所有的Shard Leaders必须已知
c) 三类消息被ShardManager监听
   消息名 说明     Cluster.MemberStatusUp 用于说明集群member的地址   LeaderStateChanged 用于说明follower拥有不同的leader   ShardRoleChanged 用于说明一个Shard的角色发生变化        d) 默认90s超时，但可配置
e) block config sub-system?" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://example.org/todo/opendaylight/" />



  </head>
  <body>
    <header class="app-header">
      <a href="http://example.org/"><img class="app-header-avatar" src="/avatar.jpg" alt="Jiang Xing" /></a>
      <h1>JiangX</h1>
      <p>Xing&#39;s personal website</p>
      <div class="app-header-social">
        
          <a target="_blank" href="https://github.com/vincent-wuhan/"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-github">
  <title>github</title>
  <path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"></path>
</svg></a>
        
      </div>
    </header>
    <main class="app-container">
      
  <article class="post">
    <header class="post-header">
      <h1 class ="post-title"></h1>
      <div class="post-meta">
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-calendar">
  <title>calendar</title>
  <rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line>
</svg>
          Jan 1, 0001
        </div>
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-clock">
  <title>clock</title>
  <circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline>
</svg>
          14 min read
        </div></div>
    </header>
    <div class="post-content">
      

<h1 id="opendaylight">Opendaylight</h1>

<h2 id="集群基础">集群基础</h2>

<h3 id="架构">架构</h3>

<p><img src="png/architecture.png" alt="Archecture" /></p>

<h4 id="数据同步">数据同步</h4>

<p>分布式数据存储同步使用Raft，远程过程调用仓库同步使用Gossip</p>

<p><img src="png/data_synchronization.png" alt="DataSynchronization" /></p>

<h4 id="分部署数据存储-distributed-data-store">分部署数据存储(Distributed Data Store)</h4>

<p><img src="./png/distributed_data_store.png" alt="Distribute Data Store" /></p>

<ul>
<li>HA</li>
</ul>

<p><img src="./png/HA.png" alt="HA" /></p>

<ul>
<li>Replication</li>
</ul>

<p>a) 按序复制(Journal replication)
<img src="png/journal_replication.png" alt="Journal replication" /></p>

<p>b) 快照型复制(Snapshot replication)
<img src="png/snapshot_replication.png" alt="Snapshot replication" /></p>

<ul>
<li>Durability/Recovery</li>
</ul>

<p><img src="png/Recovery.png" alt="Recovery" /></p>

<ul>
<li>数据存储流程</li>
</ul>

<p>Startup</p>

<p><img src="png/startup_flow.png" alt="startup" />
向DistributedConfigDataProviderModule发起createInstance请求，创建实例并执行监听回调(waitTillReadyLatch)。由ShardManager向Shard中进行数据存储，当数据存储Ready后，waitTillReadyLatch收到ready完成create流程</p>

<p>Recovery</p>

<p>从硬盘读取最新状态反馈给ShardManager，全部完毕后，通知waitTillReadyLatch countDown</p>

<p><img src="png/recovery_flow.png" alt="RecoveryFlow" /></p>

<p>Recovery的条件：</p>

<p>a) Recovery必须是完整的</p>

<p>b) 所有的Shard Leaders必须已知</p>

<p>c) 三类消息被ShardManager监听</p>

<table>
<thead>
<tr>
<th align="left">消息名</th>
<th align="left">说明</th>
</tr>
</thead>

<tbody>
<tr>
<td align="left">Cluster.MemberStatusUp</td>
<td align="left">用于说明集群member的地址</td>
</tr>

<tr>
<td align="left">LeaderStateChanged</td>
<td align="left">用于说明follower拥有不同的leader</td>
</tr>

<tr>
<td align="left">ShardRoleChanged</td>
<td align="left">用于说明一个Shard的角色发生变化</td>
</tr>

<tr>
<td align="left"></td>
<td align="left"></td>
</tr>
</tbody>
</table>

<p>d) 默认90s超时，但可配置</p>

<p>e) block config sub-system??</p>

<h4 id="远程过程调用对接-remote-rpc-connector">远程过程调用对接(Remote RPC Connector)</h4>

<p><img src="png/remote_rpc_connector.png" alt="Remote RPC Connector" /></p>

<ul>
<li><p>RPC Registry
类似于一个RPC注册总线，Provider注册成功的RPC被加入到RPC仓库后，启动一个Listener对RPC调用事件执行监听
<img src="png/rpc_registry.png" alt="RPC Registry" /></p></li>

<li><p>RPC Registry Replication - 基于Gossip</p></li>
</ul>

<p>RPC仓库在分布式环境下，需要实施保持注册RPC版本的实时性，因此不同节点之间需要进行实时版本更新。不同节点RPC仓库版本更新机制如下：
1）所有节点members之间RPC仓库版本透明
2) 每一秒节点members都会相互之间推送已知版本status
3) 收到其他节点推送的版本后，与本地版本对比，本地版本高于推送版本的，发送update指令升级其它节点
4) 本地版本低于推送版本的，发送status给高版本推送者，启动升级
<img src="png/RPC_Registry_Replication.png" alt="RPC Registry Replication" /></p>

<h3 id="集群模块">集群模块</h3>

<p><img src="png/cluster_modules.png" alt="Clustering Modules" /></p>

<ul>
<li>sal-akka-raft</li>
</ul>

<p>1) 在akka上实现了一套Raft算法
2) 提供了一个基类RaftActor用于用户扩展replicate state
3) 提供了一个对HashMap复制的简单demo，sal-akka-raft-example</p>

<ul>
<li>sal-distributed-datastore</li>
</ul>

<p>1) 并发DOMDataBroker
2) 分布式数据存储
3) DOMStore南向接口实现
4) 基于Sharding策略创建Shards
5) Shard Leader交互Client</p>

<ul>
<li>sal-remoterpc-connector</li>
</ul>

<p>1) RemoteRpcProvider(MD-SAL默认的远程过程调用的提供者)
2) BucketStore(基于Gossip进行状态复制的框架)
3) RpcBroker(回调远程rpc)</p>

<h2 id="cluster-nodes">cluster nodes</h2>

<ul>
<li>建议三节点搭建集群
OpenDaylight集群特性要求集群节点的大多数节点都是up的，而双节点集群1个节点不代表多数
两节点搭建的集群当两个节点都宕机时，集群就失效</li>
</ul>

<table>
<thead>
<tr>
<th align="left">节点IP</th>
<th align="left">说明</th>
</tr>
</thead>

<tbody>
<tr>
<td align="left">10.190.49.26</td>
<td align="left">leader, member-1</td>
</tr>

<tr>
<td align="left">10.190.49.31</td>
<td align="left">follower, member-2</td>
</tr>

<tr>
<td align="left">10.190.49.32</td>
<td align="left">follower, member-3</td>
</tr>

<tr>
<td align="left">10.190.49.74</td>
<td align="left">backup, member-4</td>
</tr>

<tr>
<td align="left">10.190.49.80</td>
<td align="left">backup, member-5</td>
</tr>

<tr>
<td align="left">10.190.49.82</td>
<td align="left">backup, member-6</td>
</tr>
</tbody>
</table>

<ul>
<li><p>Clustering scripts</p>

<pre><code class="language-txt">Note

Scripts are stored in the OpenDaylight distribution/bin folder, and maintained in the distribution project repository in the folder distribution-karaf/src/main/assembly/bin/.

</code></pre></li>
</ul>

<h2 id="集群搭建">集群搭建</h2>

<h3 id="初始化集群配置脚本">初始化集群配置脚本</h3>

<p>配置节点akka.conf和module-shards.conf配置</p>

<pre><code class="language-shell">bin/configure_cluster.sh &lt;index&gt; &lt;seed_nodes_list&gt;

注：
&lt;index&gt; 1-N，代表seed_nodes_list中node的编号,表示生成节点列表中的某个节点配置
&lt;seed_nodes_list&gt; 集群节点ip地址列表，使用”,“或者空格隔开
</code></pre>

<pre><code class="language-shell">root@rancher-server:~/opendaylight-0.9.2/bin# ./configure_cluster.sh 1 10.190.49.26 10.190.49.31 10.190.49.32
################################################
##             Configure Cluster              ##
################################################
 NOTE: Cluster configuration files not found. Copying from
 /root/opendaylight-0.9.2/system/org/opendaylight/controller/sal-clustering-config/1.8.2
Configuring unique name in akka.conf
Configuring hostname in akka.conf
Configuring data and rpc seed nodes in akka.conf
modules = [
        
        {
                name = &quot;inventory&quot;
                namespace = &quot;urn:opendaylight:inventory&quot;
                shard-strategy = &quot;module&quot;
        },
        {
                name = &quot;topology&quot;
                namespace = &quot;urn:TBD:params:xml:ns:yang:network-topology&quot;
                shard-strategy = &quot;module&quot;
        },
        {
                name = &quot;toaster&quot;
                namespace = &quot;http://netconfcentral.org/ns/toaster&quot;
                shard-strategy = &quot;module&quot;
        }
]
Configuring replication type in module-shards.conf
################################################
##   NOTE: Manually restart controller to     ##
##         apply configuration.               ##
################################################
root@rancher-server:~/opendaylight-0.9.2/bin# ll ../configuration/initial/
total 20
drwxr-xr-x 2 root root 4096 Jun 13 05:44 ./
drwxr-xr-x 3 mano mano 4096 Jun 13 05:44 ../
-rw-r--r-- 1 root root 1431 Jun 13 05:44 akka.conf
-rw-r--r-- 1 root root  336 Jun 13 05:44 modules.conf
-rw-r--r-- 1 root root  555 Jun 13 05:44 module-shards.conf

</code></pre>

<p>同样，在其余节点上执行集群配置命令, seed_nodes_list保持一致，index根据节点对应的ip进行修改</p>

<pre><code class="language-shell"># node 10.190.49.31 opendaylight文件夹根目录
bin/configure_cluster.sh 2 10.190.49.26 10.190.49.31 10.190.49.32

# node 10.190.49.32 opendaylight文件夹根目录
bin/configure_cluster.sh 3 10.190.49.26 10.190.49.31 10.190.49.32
</code></pre>

<p>执行完毕后，生成的配置文件路径 opendaylight/configuration/initial/ 下</p>

<ul>
<li><p>akka.conf</p>

<pre><code class="language-txt">odl-cluster-data {
akka {
# 定义节点ip
remote {
  artery {
    enabled = off
    canonical.hostname = &quot;10.190.49.26&quot;
    canonical.port = 2550
  }
  netty.tcp {
    hostname = &quot;10.190.49.26&quot;
    port = 2550
  }
  # when under load we might trip a false positive on the failure detector
  # transport-failure-detector {
    # heartbeat-interval = 4 s
    # acceptable-heartbeat-pause = 16s
  # }
}

# 集群节点列表，seed_nodes_list
cluster {
  # Remove &quot;.tcp&quot; when using artery.
  seed-nodes = [&quot;akka.tcp://opendaylight-cluster-data@10.190.49.26:2550&quot;,
                &quot;akka.tcp://opendaylight-cluster-data@10.190.49.31:2550&quot;,
                &quot;akka.tcp://opendaylight-cluster-data@10.190.49.32:2550&quot;]

  # index, opendaylight用roles的值作为节点索引
  roles = [&quot;member-1&quot;]

}

# 持久化存储，journal定义存储数据库
persistence {
  # By default the snapshots/journal directories live in KARAF_HOME. You can choose to put it somewhere else by
  # modifying the following two properties. The directory location specified may be a relative or absolute path. 
  # The relative path is always relative to KARAF_HOME.

  # snapshot-store.local.dir = &quot;target/snapshots&quot;
  # journal.leveldb.dir = &quot;target/journal&quot;

  journal {
    leveldb {
      # Set native = off to use a Java-only implementation of leveldb.
      # Note that the Java-only version is not currently considered by Akka to be production quality.

      # native = off
    }
  }
}
}
}

</code></pre></li>

<li><p>modules.conf</p>

<pre><code class="language-txt">定义modules的配置，包括module名称、namespace和shard-strategy(目前只支持module这种策略)
与yang模型相关, toaster模型是opendaylight的一个demo

modules = [
        
    {
            name = &quot;inventory&quot;
            namespace = &quot;urn:opendaylight:inventory&quot;
            shard-strategy = &quot;module&quot;
    },
    {
            name = &quot;topology&quot;
            namespace = &quot;urn:TBD:params:xml:ns:yang:network-topology&quot;
            shard-strategy = &quot;module&quot;
    },
    {
            name = &quot;toaster&quot;
            namespace = &quot;http://netconfcentral.org/ns/toaster&quot;
            shard-strategy = &quot;module&quot;
    }
]

</code></pre></li>

<li><p>fmodules-shards.con</p></li>
</ul>

<p>该文件定义了module和shards的映射存储关系
shards的定义的replicas即节点副本，将module的数据分片存储到replicas列表的节点集群中,类似于Elasticsearch集群数据分片副本的概念</p>

<pre><code class="language-txt">module-shards = [
        {
                name = &quot;default&quot;
                shards = [
                        {
                                name = &quot;default&quot;
                                replicas = [&quot;member-1&quot;,
                                &quot;member-2&quot;,
                                &quot;member-3&quot;]
                        }
                ]
        },
        {
                name = &quot;inventory&quot;
                shards = [
                        {
                                name=&quot;inventory&quot;
                                replicas = [&quot;member-1&quot;,
                                &quot;member-2&quot;,
                                &quot;member-3&quot;]
                        }
                ]
        },
        {
                name = &quot;topology&quot;
                shards = [
                        {
                                name=&quot;topology&quot;
                                replicas = [&quot;member-1&quot;,
                                &quot;member-2&quot;,
                                &quot;member-3&quot;]
                        }
                ]
        },
        {
                name = &quot;toaster&quot;
                shards = [
                        {
                                name=&quot;toaster&quot;
                                replicas = [&quot;member-1&quot;,
                                &quot;member-2&quot;,
                                &quot;member-3&quot;]
                        }
                ]
        }
]

</code></pre>

<h3 id="karaf部署集群feature">karaf部署集群feature</h3>

<p>在opendaylight/bin目录下运行karaf.sh脚本,该脚本依赖系统的java环境</p>

<pre><code class="language-shell">JAVA_MAX_MEM=4G JAVA_MAX_PERM_MEM=512m ./karaf
</code></pre>

<p>执行后的结果如下, 进入karaf的命令行，安装odl-mdsal-clustering feature</p>

<pre><code class="language-shell">root@rancher-server:~/opendaylight-0.9.2/bin# JAVA_MAX_MEM=4G JAVA_MAX_PERM_MEM=512m ./karaf
karaf: JAVA_HOME not set; results may vary
Apache Karaf starting up. Press Enter to open the shell now...
100% [========================================================================]

Karaf started in 0s. Bundle stats: 10 active, 10 total
                                                                                           
    ________                       ________                .__  .__       .__     __       
    \_____  \ ______   ____   ____ \______ \ _____  ___.__.|  | |__| ____ |  |___/  |_     
     /   |   \\____ \_/ __ \ /    \ |    |  \\__  \&lt;   |  ||  | |  |/ ___\|  |  \   __\    
    /    |    \  |_&gt; &gt;  ___/|   |  \|    `   \/ __ \\___  ||  |_|  / /_/  &gt;   Y  \  |      
    \_______  /   __/ \___  &gt;___|  /_______  (____  / ____||____/__\___  /|___|  /__|      
            \/|__|        \/     \/        \/     \/\/            /_____/      \/          
                                                                                           

Hit '&lt;tab&gt;' for a list of available commands
and '[cmd] --help' for help on a specific command.
Hit '&lt;ctrl-d&gt;' or type 'system:shutdown' or 'logout' to shutdown OpenDaylight.

opendaylight-user@root&gt;feature:install odl-mdsal-clustering
</code></pre>

<ul>
<li>karaf
<br /></li>
</ul>

<p>Karaf是Apache旗下的一个基于OSGi的运行环境，提供一个轻量级的OSGi容器用于部署各种组件和应用程序。</p>

<p>Karaf的安装目录机构如下，从中可以看出，整个Opendaylight的release安装包就是采用的这种安装结构，可以说Opendaylight正式通过Karaf完成部署的。</p>

<table>
<thead>
<tr>
<th align="left">目录</th>
<th align="left">说明</th>
</tr>
</thead>

<tbody>
<tr>
<td align="left">/bin</td>
<td align="left">启动脚本</td>
</tr>

<tr>
<td align="left">/etc</td>
<td align="left">初始化文件</td>
</tr>

<tr>
<td align="left">/data</td>
<td align="left">工作目录</td>
</tr>

<tr>
<td align="left">/cache</td>
<td align="left">OSGi框架包缓存</td>
</tr>

<tr>
<td align="left">/generated-bundles</td>
<td align="left">部署使用的临时文件夹</td>
</tr>

<tr>
<td align="left">/log</td>
<td align="left">日志文件</td>
</tr>

<tr>
<td align="left">/deploy</td>
<td align="left">热部署目录</td>
</tr>

<tr>
<td align="left">/instances</td>
<td align="left">含有子实例的目录</td>
</tr>

<tr>
<td align="left">/lib</td>
<td align="left">包含引导库</td>
</tr>

<tr>
<td align="left">/lib/ext</td>
<td align="left">JRE扩展目录</td>
</tr>

<tr>
<td align="left">/lib/endorsed</td>
<td align="left">赞同库目录</td>
</tr>

<tr>
<td align="left">/system</td>
<td align="left">OSGi包库，作为一个Maven2存储库</td>
</tr>
</tbody>
</table>

<p>Karaf特性：</p>

<ol>
<li>热部署
支持OSGI bundles热部署，监测[home]/deploy目录下jar包，变化时实时做出响应</li>
<li>动态配置
[home]/etc目录下的配置文件更新实时生效</li>
<li>日志系统
Log4J日志集中管理</li>
<li>供应(Provisioning)
库或应用支持本地下载、安装和启动？</li>
<li>原生OS整合
以服务的形式运行与操作系统OS中</li>
<li>扩展的内核控制台
Karaf的内核控制台可管理服务、应用或库,并且通过部署新命令已扩展新功能</li>
<li>远程访问
SSH连接Karaf进行远程控制台操作</li>
<li>基于JAAS的安全框架</li>
<li>管理实例
多实例管理，通过控制台创建、删除、启动、终止Karaf的实例</li>
</ol>

<p>常用命令行</p>

<table>
<thead>
<tr>
<th align="left">命令</th>
<th align="left">说明</th>
</tr>
</thead>

<tbody>
<tr>
<td align="left">system:shutdown/logout/<ctrl-d></td>
<td align="left">关闭OpenDaylight</td>
</tr>

<tr>
<td align="left">feature:install</td>
<td align="left">安装特性</td>
</tr>

<tr>
<td align="left">feature:list</td>
<td align="left">查看特性列表</td>
</tr>

<tr>
<td align="left">feature:info</td>
<td align="left">查看特性</td>
</tr>

<tr>
<td align="left">feature:start</td>
<td align="left">启动特性</td>
</tr>

<tr>
<td align="left">feature:stop</td>
<td align="left">关闭特性</td>
</tr>

<tr>
<td align="left">feature:uninstall</td>
<td align="left">卸载特性</td>
</tr>
</tbody>
</table>

<p>举例：</p>

<pre><code class="language-shell"># 安装Opendaylight集群feature
opendaylight-user@root&gt;feature:install odl-mdsal-clustering

# 查看feature列表，使用grep进行筛选
opendaylight-user@root&gt;feature:list|grep clustering
odl-mdsal-clustering                                            │ 1.8.2            │ x        │ Started     │ odl-mdsal-clustering                                            │ odl-mdsal-clustering
odl-clustering-test-app                                         │ 1.8.2            │          │ Started     │ odl-clustering-test-app                                         │ odl-clustering-test-app
odl-akka-clustering-2.5                                         │ 3.1.6            │          │ Started     │ odl-akka-clustering-2.5                                         │ Akka Clustering
odl-mdsal-clustering-commons                                    │ 1.8.2            │          │ Started     │ odl-controller-1.8.2                                            │ odl-mdsal-clustering-commons

# 查看feature info
opendaylight-user@root&gt;feature:info odl-mdsal-clustering
Feature odl-mdsal-clustering 1.8.2
Description:
  odl-mdsal-clustering
Details:
  OpenDaylight is leading the transformation to Open Software Defined Networking (SDN). For more information, please see https://www.opendaylight.org
Feature has no configuration
Feature has no configuration files
Feature depends on:
  odl-mdsal-broker 1.8.2
Feature has no bundles.
Feature has no conditionals.

# 关闭feature, 状态由Started变为了Resolved
opendaylight-user@root&gt;feature:stop odl-mdsal-clustering
opendaylight-user@root&gt;feature:list|grep odl-mdsal-clustering
odl-mdsal-clustering                                            │ 1.8.2            │ x        │ Resolved    │ odl-mdsal-clustering                                            │ odl-mdsal-clustering
odl-mdsal-clustering-commons                                    │ 1.8.2            │          │ Started     │ odl-controller-1.8.2                                            │ odl-mdsal-clustering-commons

# 启动feature, 状态重新变为Started
opendaylight-user@root&gt;feature:start odl-mdsal-clustering
opendaylight-user@root&gt;feature:list|grep odl-mdsal-clustering
odl-mdsal-clustering                                            │ 1.8.2            │ x        │ Started     │ odl-mdsal-clustering                                            │ odl-mdsal-clustering
odl-mdsal-clustering-commons                                    │ 1.8.2            │          │ Started     │ odl-controller-1.8.2                                            │ odl-mdsal-clustering-commons

# 卸载feature(貌似此处有个bug，unistall odl-mdsal-clustering后，该feature的状态仍然是Started,正常的应该是Uninstalled)
opendaylight-user@root&gt;feature:list|grep odl-mdsal-clustering
odl-mdsal-clustering                                            │ 1.8.2            │          │ Started     │ odl-mdsal-clustering                                            │ odl-mdsal-clustering
odl-mdsal-clustering-commons                                    │ 1.8.2            │          │ Started     │ odl-controller-1.8.2                                            │ odl-mdsal-clustering-commons
opendaylight-user@root&gt;feature:uninstall odl-mdsal-clustering  
Error executing command: Feature named 'odl-mdsal-clustering/0.0.0' is not installed

</code></pre>

<h3 id="数据持久化和备份">数据持久化和备份</h3>

<ul>
<li>配置开关脚本</li>
</ul>

<p>集群默认是开启数据持久化存储的，特殊情况下需要修改配置，可执行如下脚本。</p>

<pre><code class="language-shell">bin/set_persistence.sh &lt;on/off&gt;
</code></pre>

<p>该脚本可在任意时刻执行生效，无论控制器是否启动.</p>

<ul>
<li>备份恢复存储数据</li>
</ul>

<p>在单个节点上调用RPC进行本地数据备份，RPC入参为一个文件名.调用的API为backup-datastore，详情见下方API说明。</p>

<h3 id="odl-mdsal-clustering-feature的依赖">odl-mdsal-clustering feature的依赖</h3>

<h4 id="如何理解feature-bundles的关系">如何理解feature、bundles的关系</h4>

<p>一个bundles理解为具体的功能实现，类似于container
一个feature类似于一个分组，该分组可以纯粹是其他分组(feature)的聚合，即不包含任何bundles, 而是仅仅描述多个features的聚合关系，类似于oom的top chart.一个feature也可以包含有具体的bundles，类似于子charts</p>

<h4 id="odl-mdsal-clustering-依赖">odl-mdsal-clustering 依赖</h4>

<p>odl-mdsal-clustering feature的唯一依賴 odl-mdsal-broker, 前面使用configure_cluster.sh腳本生成的inital/akka.conf,factory/akka.conf,modules.conf, module-shards.conf等配置文件就是为odl-mdsal-broker feature的配置文件</p>

<table>
<thead>
<tr>
<th align="left">feature</th>
<th align="left">depends</th>
<th align="left">bunldles</th>
<th align="left">config</th>
</tr>
</thead>

<tbody>
<tr>
<td align="left">odl-mdsal-clustering</td>
<td align="left">odl-mdsal-broker</td>
<td align="left"></td>
<td align="left"></td>
</tr>

<tr>
<td align="left">odl-mdsal-broker</td>
<td align="left">odl-mdsal-remoterpc-connector 1.8.2<br> odl-mdsal-distributed-datasotre 1.8.2<br></td>
<td align="left"></td>
<td align="left">configuration/initial/akka.conf<br> configuration/factory/akka.conf<br> configuration/initial/module-shards.conf<br> configuration/initial/modules.conf<br> etc/org.opendaylight.controller.cluster.datastore.cfg</td>
</tr>

<tr>
<td align="left">odl-mdsal-remoterpc-connector</td>
<td align="left">odl-akka-leveldb-0.7[3.1.0,4]<br> odl-mdsal-broker-local 1.8.2<br> odl-mdsal-clustering-commons 1.8.2<br></td>
<td align="left">mvn:org.opendaylight.controller/sal-remoterpc-connector/1.8.2</td>
<td align="left"></td>
</tr>

<tr>
<td align="left">odl-mdsal-distributed-datastore</td>
<td align="left">odl-mdsal-broker-local 1.8.2<br> odl-mdsal-clustering-commons 1.8.2<br> wrap 0.0.0<br></td>
<td align="left">mvn:org.opendaylight.controller/cds-access-api/1.4.2<br>mvn:org.opendaylight.controller/cds-access-client/1.4.2<br>mvn:org.opendaylight.controller/cds-dom-api/1.4.2<br>mvn:org.opendaylight.controller/sal-distributed-datastore/1.8.2<br>wrap:mvn:net.java.dev.stax-utils/stax-utils/20070216<br> wrap:mvn:com.bea.xml/jsr173-ri/1.0<br>mvn:org.apache.commons/commons-text/1.1<br> mvn:org.opendaylight.controller/sal-cluster-admin-api/1.8.2<br> mvn:org.opendaylight.controller/sal-cluster-admin-impl/1.8.2<br></td>
<td align="left"></td>
</tr>

<tr>
<td align="left">odl-mdsal-broker-local</td>
<td align="left">odl-lmax-3 [3.1.0,4)<br> odl-yangtools-codec [2.0.3,3)<br> odl-mdsal-binding-dom-adapter 2.5.2<br> odl-config-netty 0.9.2<br> odl-controller-mdsal-common 1.8.2<br> odl-mdsal-dom 2.5.2<br> odl-mdsal-eos-dom 2.5.2<br> odl-mdsal-eos-binding 2.5.2<br> odl-mdsal-singleton-dom 2.5.2<br> wrap 0.0.0<br></td>
<td align="left">mvn:org.opendaylight.controller/blueprint/0.9.2 start-level=40<br> mvn:org.opendaylight.controller/sal-core-api/1.8.2<br> mvn:org.opendaylight.controller/sal-core-spi/1.8.2<br> mvn:org.opendaylight.controller/sal-broker-impl/1.8.2<br> mvn:org.opendaylight.controller/sal-binding-api/1.8.2<br> mvn:org.opendaylight.controller/sal-binding-broker-impl/1.8.2<br> mvn:org.opendaylight.controller/sal-binding-util/1.8.2<br> mvn:org.opendaylight.controller/sal-connector-api/1.8.2<br> mvn:org.opendaylight.controller/sal-inmemory-datastore/1.8.2<br> mvn:org.opendaylight.controller/sal-core-compat/1.8.2<br> mvn:org.apache.aries/org.apache.aries.util/1.1.3<br> mvn:org.osgi/org.osgi.service.event/1.3.1<br></td>
<td align="left"></td>
</tr>

<tr>
<td align="left">odl-mdsal-clustering-commons</td>
<td align="left">odl-akka-system-2.5 [3.1.0,4)<br> odl-akka-persistence-2.5 [3.1.0,4)<br> odl-akka-clustering-2.5 [3.1.0,4)<br> odl-mdsal-broker-local 1.8.2<br></td>
<td align="left">mvn:org.opendaylight.controller/sal-clustering-commons/1.8.2<br> mvn:commons-lang/commons-lang/2.6<br> mvn:javax.servlet/javax.servlet-api/3.1.0<br> mvn:io.dropwizard.metrics/metrics-graphite/3.1.2<br> mvn:org.opendaylight.controller/sal-akka-raft/1.8.2<br> mvn:commons-io/commons-io/2.6<br> mvn:io.dropwizard.metrics/metrics-core/3.1.2<br></td>
<td align="left"></td>
</tr>

<tr>
<td align="left">odl-akka-leveldb-0.7</td>
<td align="left">odl-guava-23 3.1.6<br> wrap 0.0.0<br></td>
<td align="left">wrap:mvn:org.iq80.leveldb/leveldb/0.7<br> mvn:org.fusesource.leveldbjni/leveldbjni-all/1.8-odl<br></td>
<td align="left"></td>
</tr>
</tbody>
</table>

<h3 id="osgi-todo">OSGi (TODO)</h3>

<h3 id="yang">YANG</h3>

<p>YANG的语法类似C和C++</p>

<h4 id="语法规则">语法规则</h4>

<ul>
<li>注释</li>
</ul>

<ol>
<li>单行使用&rdquo;//&rdquo;</li>
<li>块注释&rdquo;/* */&rdquo;</li>
</ol>

<ul>
<li>Tokens</li>
</ul>

<ol>
<li>大小写敏感</li>
<li>文档中自定义的关键字</li>
<li>前缀标识符+ &ldquo;:&ldquo;+一个语言扩展关键字)、一个字符串、一个分号(&rdquo;:&ldquo;)和括弧(&rdquo;{&ldquo;或&rdquo;}&ldquo;)</li>
</ol>

<ul>
<li>标识符
通过名称来识别不同类型的YANG项目。</li>
</ul>

<ol>
<li>由一个大写或小写ASCII字符或下划线打头，后跟0个或多个ASCII字符、数字、下划线、连字符和点</li>
<li>大小写敏感</li>
<li>最长64个字符</li>
<li>可以是字符串，带引号或不带引号皆可</li>
<li>在依赖被定义的YANG项目的类型的命名空间内有效</li>
<li>在同一个命名空间内具有唯一性</li>
</ol>

<ul>
<li>声明</li>
</ul>

<p>每个声明由一个关键字打头，随后是0或1个参数(argument), 再随后是一个分号(&ldquo;;&rdquo;)或括号(&ldquo;{}&rdquo;)封装的子声明块。</p>

<pre><code class="language-txt">statement = keyword[argument](&quot;;&quot;/&quot;{&quot;*statement&quot;})
// argument是一个字符串
</code></pre>

<ul>
<li>扩展extension</li>
</ul>

<ol>
<li>一个模块可使用extension关键字引入YANG扩展</li>
<li>扩展可由其他带有import声明的模块导入</li>
<li>扩展的关键字前必须使用扩展模块导入的前缀进行限定(扩展的关键字前必须加上定义该关键字的模块前缀)</li>
<li>子模块无法包括父模块，因此一个模块中需要向子模块暴露细节的扩展，必须在子模块中定义</li>
<li>对于未知生命，YANG编译器会忽视</li>
</ol>

<h3 id="api">API</h3>

<h4 id="cluster-admin-api">cluster-admin API</h4>

<h5 id="yang-sal-cluster-admin-api">YANG (sal-cluster-admin-api)</h5>

<p>cluster-admin API的yang模型定义，模型定义文件cluster-admin.yang</p>

<pre><code class="language-yang">// module 声明模块
module cluster-admin {

    /* 可选的模块开发所用的YANG版本声明,包括YANG版本声明、命名空间声明和前缀声明*/
    /* namespace所有标识符命名空间的定义，是命名空间的URI */
    /* prefix, 用以访问一个模块，前缀:待引用对象,所有的前缀在此模块或子模块中必须唯一 */
    yang-version 1;
    namespace &quot;urn:opendaylight:params:xml:ns:yang:controller:md:sal:cluster:admin&quot;;
    prefix &quot;cluster-admin&quot;;

    description
        &quot;This module contains YANG RPC definitions for administering a cluster.&quot;;

    // 修订历史，包括修订声明，以revision加YYYY-MM-DD的格式，后接一段子声明
    revision &quot;2015-10-13&quot; {
        description &quot;Initial revision.&quot;;
    }

    // typedef 自定义类型声明
    // 数据存储树，config和operational
    typedef data-store-type {
        // type 类型声明
        type enumeration {
            enum config {
                value 1;
            }
            enum operational {
                value 2;
            }
        }
    }

    // grouping组声明，定义一块可复用的节点。可在模块中本地使用、在包含其的模块中使用、在其他导入它的模块中使用
    grouping shard-operation-result {
        // leaf 叶子节点声明
        leaf shard-name {
            type string;
        }

        leaf data-store-type {
            type data-store-type;
        }

        leaf succeeded {
            type boolean;
        }

        leaf error-message {
            type string;
        }
    }

    grouping shard-result-output {
        // list 列表声明, 定义在概要树的一个内部数据节点。一个列表节点可能存在于数据树的多个实例中
        list shard-result {
            // key声明， 若列表代表配置，则key声明必须出现，其他情况可选
            key &quot;shard-name data-store-type&quot;;

            // uses 使用声明, 用以引入一个grouping定义，使用组名作为参数
            uses shard-operation-result;

            description &quot;The list of results, one per shard&quot;;
        }
    }

    grouping member-voting-states-input {
        list member-voting-state {
            leaf member-name {
                type string;
            }

            leaf voting {
                type boolean;
            }

            description &quot;The list of member voting states&quot;;
        }
    }

    // rpc 远程过程调用声明
    rpc add-shard-replica {
        // input可选项，用以定义RPC操作的输入参数
        input {
            // leaf定义入参名称 shard-name
            leaf shard-name {
                /* mandatory定义该参数属性，true为强制必选，false为参数可选*/
                /* type为参类型 */
                /* description为参数说明 */
                mandatory true;
                type string;
                description &quot;The name of the shard for which to create a replica.&quot;;
            }

            leaf data-store-type {
                mandatory true;
                // 该参数类型为一个枚举类型
                type data-store-type;
                description &quot;The type of the data store to which the replica belongs&quot;;
            }
        }

        // rpc函数说明
        description &quot;Adds a replica of a shard to this node and joins it to an existing cluster. The shard must
            already have a module configuration defined for it and there must already be a shard existing on
            another node with a leader. This RPC first contacts peer member seed nodes searching for a shard.
            When found, an AddServer message is sent to the shard leader and applied as described in the Raft
            paper.&quot;;
    }

    rpc remove-shard-replica {
        input {
            leaf shard-name {
                mandatory true;
                type string;
                description &quot;The name of the shard for which to remove the replica.&quot;;
            }

            leaf member-name {
                mandatory true;
                type string;
                description &quot;The cluster member from which the shard replica should be removed&quot;;
            }

            leaf data-store-type {
                mandatory true;
                type data-store-type;
                description &quot;The type of the data store to which the replica belongs&quot;;
            }
        }

        description &quot;Removes an existing replica of a shard from this node via the RemoveServer mechanism as
            described in the Raft paper.&quot;;
    }

    rpc make-leader-local {
        input {
            leaf shard-name {
                mandatory true;
                type string;
                description &quot;The name of the shard for which to move the leader to the local node&quot;;
            }

            leaf data-store-type {
                mandatory true;
                type data-store-type;
                description &quot;The type of the data store to which the shard belongs&quot;;
            }
        }

        description &quot;Attempts to move the shard leader of the given module based shard to the local node.
                The rpc returns a response after handling of the underlying MakeLeaderLocal message completes.
                This operation fails if there is no current shard leader due to lack of network connectivity or
                a cluster majority. In addition, if the local node is not up to date with the current leader,
                an attempt is made to first sync the local node with the leader. If this cannot be achieved
                within two election timeout periods the operation fails.&quot;;
    }

    rpc add-prefix-shard-replica {
        input {
            leaf shard-prefix {
                mandatory true;
                type instance-identifier;
            }

            leaf data-store-type {
                mandatory true;
                type data-store-type;
                description &quot;The type of the data store to which the replica belongs&quot;;
            }
        }

        description &quot;Adds a replica of a shard to this node and joins it to an existing cluster. There must already be
                    a shard existing on another node with a leader. This RPC first contacts peer member seed nodes
                    searching for a shard. When found, an AddServer message is sent to the shard leader and applied as
                    described in the Raft paper.&quot;;
    }

    rpc remove-prefix-shard-replica {
        input {
            leaf shard-prefix {
                mandatory true;
                type instance-identifier;
            }
            leaf member-name {
                mandatory true;
                type string;
                description &quot;The cluster member from which the shard replica should be removed&quot;;
            }

            leaf data-store-type {
                mandatory true;
                type data-store-type;
                description &quot;The type of the data store to which the replica belongs&quot;;
            }
        }

        description &quot;Removes an existing replica of a prefix shard from this node via the RemoveServer mechanism as
                    described in the Raft paper.&quot;;
    }

    rpc add-replicas-for-all-shards {
        // output 可选项，用以定义RPC操作的输出参数
        output {
            uses shard-result-output;
        }

        description &quot;Adds replicas on this node for all currently defined shards. This is equivalent to issuing
            an add-shard-replica RPC for all shards.&quot;;
    }

    rpc remove-all-shard-replicas {
        input {
            leaf member-name {
                mandatory true;
                type string;
                description &quot;The cluster member from which the shard replicas should be removed&quot;;
            }
        }

        output {
            uses shard-result-output;
        }

        description &quot;Removes replicas for all shards on this node. This is equivalent to issuing
            a remove-shard-replica for all shards and essentially removes this node from a cluster.&quot;;
    }

    rpc change-member-voting-states-for-shard {
        input {
            leaf shard-name {
                mandatory true;
                type string;
                description &quot;The name of the shard for which to change voting state.&quot;;
            }

            leaf data-store-type {
                mandatory true;
                type data-store-type;
                description &quot;The type of the data store to which the shard belongs&quot;;
            }

            uses member-voting-states-input;
        }

        description &quot;Changes the voting states, either voting or non-voting, of cluster members for a shard.
            Non-voting members will no longer participate in leader elections and consensus but will be
            replicated. This is useful for having a set of members serve as a backup cluster in case the
            primary voting cluster suffers catastrophic failure. This RPC can be issued to any cluster member
            and will be forwarded to the leader.&quot;;
    }

    rpc change-member-voting-states-for-all-shards {
        input {
            uses member-voting-states-input;
        }

        output {
            uses shard-result-output;
        }

        description &quot;Changes the voting states, either voting or non-voting, of cluster members for all shards.
            Non-voting members will no longer participate in leader elections and consensus but will be
            replicated. This is useful for having a set of members serve as a backup cluster in case the
            primary voting cluster suffers catastrophic failure. This RPC can be issued to any cluster member
            and will be forwarded to the leader.&quot;;
    }

    rpc flip-member-voting-states-for-all-shards {
        output {
            uses shard-result-output;
        }

        description &quot;Flips the voting states of all cluster members for all shards, such that if a member
            was voting it becomes non-voting and vice versa.&quot;;
    }

    rpc backup-datastore {
        input {
            leaf file-path {
              type string;
              description &quot;The path and name of the file in which to store the backup.&quot;;
            }
        }

        description &quot;Creates a backup file of the datastore state&quot;;
    }

    // 获取当前请求的module-shard角色
    rpc get-shard-role {
        input {
            leaf shard-name {
                mandatory true;
                type string;
                description &quot;The name of the shard for which to create a replica.&quot;;
            }

            // 填写&quot;operational&quot;
            leaf data-store-type {
                mandatory true;
                type data-store-type;
                description &quot;The type of the data store to which the replica belongs&quot;;
            }
        }

        output {
            // 主节点返回&quot;Leader&quot;,从节点返回&quot;Follower&quot;
            leaf role {
                type string;
                description &quot;Current role for the given shard, if not present the shard currently doesn't have a role&quot;;
            }
        }

        description &quot;Returns the current role for the requested module shard.&quot;;
    }

    rpc get-prefix-shard-role {
        input {
            leaf shard-prefix {
                mandatory true;
                type instance-identifier;
            }

            leaf data-store-type {
                mandatory true;
                type data-store-type;
                description &quot;The type of the data store to which the replica belongs&quot;;
            }
        }

        output {
            leaf role {
                type string;
                description &quot;Current role for the given shard, if not present the shard currently doesn't have a role&quot;;
            }
        }

        description &quot;Returns the current role for the requested module shard.&quot;;
    }
}

</code></pre>

<h5 id="impl-sal-cluster-admin-impl">IMPL (sal-cluster-admin-impl)</h5>

<h5 id="api测试">API测试</h5>

<ul>
<li>get-shard-role</li>
</ul>

<p>功能：获取shard角色</p>

<p>说明：角色分为Leader和Follower, 因Shard所处的Node IP地址而异。</p>

<p>URI: /restconf/operations/cluster-admin:get-shard-role</p>

<p>NodeIP: 10.190.49.26 （主节点)
Request Body:</p>

<pre><code class="language-json">{
         &quot;input&quot;: {
             &quot;shard-name&quot;: &quot;default&quot;,
             &quot;data-store-type&quot;: &quot;operational&quot;
         }
}
</code></pre>

<p>主节点Response:</p>

<pre><code class="language-json">{
  &quot;output&quot;: {
    &quot;role&quot;: &quot;Leader&quot;
  }
}
</code></pre>

<p>从节点Response：</p>

<pre><code class="language-json">{
  &quot;output&quot;: {
    &quot;role&quot;: &quot;Follower&quot;
  }
}
</code></pre>

<ul>
<li>change-voting-states-for-all-shards</li>
</ul>

<p>功能: 改变一组shard的voting状态。</p>

<p>说明: 状态置为false时，shard不参与Leader的选举, 默认为true，参与voting。</p>

<p>URI: /restconf/operations/cluster-admin:change-member-voting-states-for-all-shards</p>

<p>Request Body</p>

<pre><code class="language-json"># 设置4、5、6节点的shard voting为false
{
  &quot;input&quot;: {
    &quot;member-voting-state&quot;: [
      {
        &quot;member-name&quot;: &quot;member-4&quot;,
        &quot;voting&quot;: false
      },
      {
        &quot;member-name&quot;: &quot;member-5&quot;,
        &quot;voting&quot;: false
      },
      {
        &quot;member-name&quot;: &quot;member-6&quot;,
        &quot;voting&quot;: false
      }
    ]
  }
}
</code></pre>

<p>Response
更细了分布式存储数据库中两棵存储树Operational和config上的数据信息, succeeded标识更新结果</p>

<pre><code class="language-json">{
  &quot;output&quot;: {
    &quot;shard-result&quot;: [
      {
        &quot;shard-name&quot;: &quot;inventory&quot;,
        &quot;data-store-type&quot;: &quot;operational&quot;,
        &quot;succeeded&quot;: true
      },
      {
        &quot;shard-name&quot;: &quot;prefix-configuration-shard&quot;,
        &quot;data-store-type&quot;: &quot;operational&quot;,
        &quot;succeeded&quot;: true
      },
      {
        &quot;shard-name&quot;: &quot;default&quot;,
        &quot;data-store-type&quot;: &quot;operational&quot;,
        &quot;succeeded&quot;: true
      },
      {
        &quot;shard-name&quot;: &quot;toaster&quot;,
        &quot;data-store-type&quot;: &quot;operational&quot;,
        &quot;succeeded&quot;: true
      },
      {
        &quot;shard-name&quot;: &quot;inventory&quot;,
        &quot;data-store-type&quot;: &quot;config&quot;,
        &quot;succeeded&quot;: true
      },
      {
        &quot;shard-name&quot;: &quot;prefix-configuration-shard&quot;,
        &quot;data-store-type&quot;: &quot;config&quot;,
        &quot;succeeded&quot;: true
      },
      {
        &quot;shard-name&quot;: &quot;toaster&quot;,
        &quot;data-store-type&quot;: &quot;config&quot;,
        &quot;succeeded&quot;: true
      },
      {
        &quot;shard-name&quot;: &quot;entity-ownership&quot;,
        &quot;data-store-type&quot;: &quot;operational&quot;,
        &quot;succeeded&quot;: true
      },
      {
        &quot;shard-name&quot;: &quot;topology&quot;,
        &quot;data-store-type&quot;: &quot;config&quot;,
        &quot;succeeded&quot;: true
      },
      {
        &quot;shard-name&quot;: &quot;topology&quot;,
        &quot;data-store-type&quot;: &quot;operational&quot;,
        &quot;succeeded&quot;: true
      },
      {
        &quot;shard-name&quot;: &quot;default&quot;,
        &quot;data-store-type&quot;: &quot;config&quot;,
        &quot;succeeded&quot;: true
      }
    ]
  }
}
</code></pre>

<ul>
<li>flip-member-voting-states-for-all-shards</li>
</ul>

<p>功能： 反转所有shards的voting状态</p>

<p>说明： 在一个active/backup集群环境中，在需要反转backup节点上shards的non-voting状态为voting和反转active节点上shards的voting状态为non-voting时使用。简言之类似主备倒换</p>

<p>在执行flip这个RPC请求时，由于有本地自选机制存在，需要防止集群分裂的情况，说明如下：</p>

<p>本地自选机制</p>

<pre><code class="language-txt">当异常断电导致所有主站voting节点宕机，flip RPC请求不得不发送给备站non-voting节点时，这种情况下因为没有shard leaders执行voting变化，收到RPC请求的节点会将non-voting状态变为voting并且自己成为leader，并保存voting变化并将变化数据复制到幸存的其他从节点。
</code></pre>

<p>防止集群分裂</p>

<pre><code class="language-txt">
当主站断电故障清除，准备恢复上电时，由于在主站宕机期间，voting状态沦为备，原主站数据库不存储断电期间的数据变化。当原来的主站拉起时，原主站集群内的voting节点仍处于voting状态，理想情况下，这些新启的节点可以连接备站，从而follow备站的leader以及同步leader的数据。但是一旦主站备战间follow建立失败，可能导致原来的单集群主备用保护模式分裂为两个独立集群。
结论：主站恢复前，要么清除主站所有数据，要么从备站上恢复最近的一次数据备份，从而防止主用恢复时集群分裂。

</code></pre>

<ul>
<li>remove-all-shard-replicas</li>
</ul>

<p>功能：将一个节点从集群中移除</p>

<p>URI: /restconf/operations/cluster-admin:remove-all-shard-replicas</p>

<pre><code class="language-json">{
  &quot;input&quot;: {
    &quot;member-name&quot;: &quot;member-6&quot;
  }
}
</code></pre>

<p>移除了节点6上的所有分片存储数据
Response:</p>

<pre><code class="language-json">{
  &quot;output&quot;: {
    &quot;shard-result&quot;: [
      {
        &quot;shard-name&quot;: &quot;inventory&quot;,
        &quot;data-store-type&quot;: &quot;operational&quot;,
        &quot;succeeded&quot;: true
      },
      {
        &quot;shard-name&quot;: &quot;prefix-configuration-shard&quot;,
        &quot;data-store-type&quot;: &quot;operational&quot;,
        &quot;succeeded&quot;: true
      },
      {
        &quot;shard-name&quot;: &quot;default&quot;,
        &quot;data-store-type&quot;: &quot;operational&quot;,
        &quot;succeeded&quot;: true
      },
      {
        &quot;shard-name&quot;: &quot;toaster&quot;,
        &quot;data-store-type&quot;: &quot;operational&quot;,
        &quot;succeeded&quot;: true
      },
      {
        &quot;shard-name&quot;: &quot;inventory&quot;,
        &quot;data-store-type&quot;: &quot;config&quot;,
        &quot;succeeded&quot;: true
      },
      {
        &quot;shard-name&quot;: &quot;prefix-configuration-shard&quot;,
        &quot;data-store-type&quot;: &quot;config&quot;,
        &quot;succeeded&quot;: true
      },
      {
        &quot;shard-name&quot;: &quot;toaster&quot;,
        &quot;data-store-type&quot;: &quot;config&quot;,
        &quot;succeeded&quot;: true
      },
      {
        &quot;shard-name&quot;: &quot;entity-ownership&quot;,
        &quot;data-store-type&quot;: &quot;operational&quot;,
        &quot;succeeded&quot;: true
      },
      {
        &quot;shard-name&quot;: &quot;topology&quot;,
        &quot;data-store-type&quot;: &quot;config&quot;,
        &quot;succeeded&quot;: true
      },
      {
        &quot;shard-name&quot;: &quot;topology&quot;,
        &quot;data-store-type&quot;: &quot;operational&quot;,
        &quot;succeeded&quot;: true
      },
      {
        &quot;shard-name&quot;: &quot;default&quot;,
        &quot;data-store-type&quot;: &quot;config&quot;,
        &quot;succeeded&quot;: true
      }
    ]
  }
}

</code></pre>

<ul>
<li>remove-shard-replica</li>
</ul>

<p>功能: 将特定分片(shard)移除</p>

<p>说明: 例如将节点member-2上的default shard的config从节点移除</p>

<p>Request Body:</p>

<pre><code class="language-json">{
  &quot;input&quot;: {
    &quot;shard-name&quot;: &quot;default&quot;,
    &quot;member-name&quot;: &quot;member-6&quot;,
    &quot;data-store-type&quot;: &quot;config&quot;
  }
}
</code></pre>

<ul>
<li>add-replicas-for-all-shards</li>
</ul>

<p>功能: 动态添加节点到集群</p>

<p>说明: 动态，实时新增节点入集群，无需修改健康节点的配置。不需要任何输入参数，但是该RPC请求需要向新增节点发起，触发该节点向集群中所有节点复制shards</p>

<p>注意: Cluster Admin API动态添加和移除shards时，并不会改变节点初始化module-shard.conf和modules.conf配置文件，而是改变的joural(数据库?)</p>

<p>向节点6执行该RPC请求，将节点6重新加入回集群，从下列执行结果来看，在节点6上重新生成了shard备份</p>

<p>Response Body</p>

<pre><code class="language-json">{
  &quot;output&quot;: {
    &quot;shard-result&quot;: [
      {
        &quot;shard-name&quot;: &quot;inventory&quot;,
        &quot;data-store-type&quot;: &quot;operational&quot;,
        &quot;succeeded&quot;: true
      },
      {
        &quot;shard-name&quot;: &quot;prefix-configuration-shard&quot;,
        &quot;data-store-type&quot;: &quot;operational&quot;,
        &quot;succeeded&quot;: true
      },
      {
        &quot;shard-name&quot;: &quot;default&quot;,
        &quot;data-store-type&quot;: &quot;operational&quot;,
        &quot;succeeded&quot;: true
      },
      {
        &quot;shard-name&quot;: &quot;toaster&quot;,
        &quot;data-store-type&quot;: &quot;operational&quot;,
        &quot;succeeded&quot;: true
      },
      {
        &quot;shard-name&quot;: &quot;inventory&quot;,
        &quot;data-store-type&quot;: &quot;config&quot;,
        &quot;succeeded&quot;: true
      },
      {
        &quot;shard-name&quot;: &quot;prefix-configuration-shard&quot;,
        &quot;data-store-type&quot;: &quot;config&quot;,
        &quot;succeeded&quot;: true
      },
      {
        &quot;shard-name&quot;: &quot;toaster&quot;,
        &quot;data-store-type&quot;: &quot;config&quot;,
        &quot;succeeded&quot;: true
      },
      {
        &quot;shard-name&quot;: &quot;entity-ownership&quot;,
        &quot;data-store-type&quot;: &quot;operational&quot;,
        &quot;succeeded&quot;: true
      },
      {
        &quot;shard-name&quot;: &quot;topology&quot;,
        &quot;data-store-type&quot;: &quot;config&quot;,
        &quot;succeeded&quot;: true
      },
      {
        &quot;shard-name&quot;: &quot;topology&quot;,
        &quot;data-store-type&quot;: &quot;operational&quot;,
        &quot;succeeded&quot;: true
      },
      {
        &quot;shard-name&quot;: &quot;default&quot;,
        &quot;data-store-type&quot;: &quot;config&quot;,
        &quot;succeeded&quot;: true
      }
    ]
  }
}

</code></pre>

<ul>
<li>backup-datastore</li>
</ul>

<p>功能: 对节点数据进行备份
说明: 在当前节点上进行数据备份
URI: /restconf/operations/cluster-admin:backup-datastore</p>

<p>Request Body</p>

<p>备份文件参考官网说明需要放在$KARAF_HOME/clustered-datastore-restore目录下，文件名称任意
首次安装是，目录和文件都不存在，需要自己手动创建
clustered-datastore-restore目录与journal和snapshots平级
由于备份文件是每个节点独立的，为了保证各个节点数据在恢复时都不会出现数据缺失的情况，最好每个节点都能进行数据备份比较保险</p>

<p>备份文件准备完毕后，向每一个节点发起RPC请求创建持久化存储</p>

<p>Request</p>

<pre><code class="language-json">
{
  &quot;input&quot;: {
    &quot;file-path&quot;: &quot;/root/opendaylight-0.9.2/clustered-datastore-restore/datastore_backup&quot;
  }
}
</code></pre>

<p>Response</p>

<pre><code class="language-json">{
  &quot;output&quot;: {}
}
</code></pre>

    </div>
    <div class="post-footer">
      
    </div>
  </article>

    </main>
  </body>
</html>
